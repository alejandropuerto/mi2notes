
\section{Kernel PCA}

We apply standard linear PCA on the \emph{transformed} version of the data
$
\left\{
\vec{\phi}_{(\vec{x}^{(\alpha)})}
\right\}_{\alpha=1}^{p}
$.
We will first assume we have $\vec{\phi}_{(\vec{x})}$ 
but we will eventually turn to $K_{\alpha \beta}$ 
which we can actually obtain.\\

\underline{Remark:}
A difference between these notes and the lecture slides is that 
the lecture slides employ ``identity'' as the mapping. 
This is why you don't see $\phi$ in the derivations of Kernel PCA in the slides but rather see $\vec x$ used directly.\\

\begin{enumerate}
\item PCA assumes its input is centered. 
$\frac{1}{p} \sum^{p}_{\alpha=1} \vec{\phi}_{(\vec{x}^{(\alpha)})}$\\
Centering $\vec X$ does not guranatee it stays centered after transformation.
Therefore, there is no need to center $\vec X$ beforehand.

\item Compute the covariance matrix $\vec C_{\phi}$ for $\vec{\phi}_{(\vec{x})}$:


\begin{equation} \label{eq:cov}
\vec C_{\phi} = \frac{1}{p} \sum_{\alpha=1}^{p} \vec{\phi}_{(\vec{x}^{(\alpha)})} \vec{\phi}^{\top}_{(\vec{x}^{(\alpha)})}
\end{equation}

\item Solve the eigenvalue problem:

\begin{equation} \label{eq:eig}
\vec C_{\phi} \, \vec e = \lambda \vec e
\end{equation}

Each eigenvector $\vec e_i$ with corresponding $\lambda_i \ne 0$ lies in the span of 
$
\left\{
\vec{\phi}_{(\vec{x}^{(\alpha)})}
\right\}_{\alpha=1}^{p}.
$

Consequently, there exists a set of coefficients (i.e. a coefficient for each transformed observation)
$
\left\{
a^{(\alpha)}
\right\}_{\alpha=1}^{p}
$, which satisfies the following:

\begin{equation}
\label{eq:ephi}
\vec e = \sum^{p}_{\beta=1} a^{(\beta)} \vec{\phi}_{(\vec{x}^{(\beta)})}
\end{equation}

Eq.\ref{eq:ephi} tells us that we can describe $\vec e$ in terms of the transformed observations (a weighted summation of $\phi$'s).
 The use of the index $\beta$ is only to avoid collisions with $\alpha$ later.

Substituting Eq.\ref{eq:cov} and Eq.\ref{eq:ephi} into the eignevalue problem Eq.\ref{eq:eig}:

\begin{equation*}
\underbrace{\frac{1}{p} \sum_{\alpha=1}^{p} \vec{\phi}_{(\vec{x}^{(\alpha)})} \vec{\phi}^{\top}_{(\vec{x}^{(\alpha)})} 
}_{=\,\vec C_{\phi}}
 \, 
\underbrace{\sum^{p}_{\beta=1} a^{(\beta)} \vec{\phi}_{(\vec{x}^{(\beta)})}}_{=\,\vec e}
 = \lambda \;\,
\underbrace{\sum^{p}_{\beta=1} a^{(\beta)} \vec{\phi}_{(\vec{x}^{(\beta)})}}_{=\,\vec e}
\end{equation*}

After rearranging the terms we get:
\begin{equation} \label{eq:eig2}
\frac{1}{p} \sum_{\alpha=1}^{p} \sum^{p}_{\beta=1} 
a^{(\beta)} \vec{\phi}_{(\vec{x}^{(\alpha)})}
\underbrace{
 \vec{\phi}^{\top}_{(\vec{x}^{(\alpha)})} \,  \vec{\phi}_{(\vec{x}^{(\beta)})}
}_{\substack{\text{scalar product}\\ = K_{\alpha\beta}}}
 = \lambda 
\sum^{p}_{\beta=1} a^{(\beta)} \vec{\phi}_{(\vec{x}^{(\beta)})}
\end{equation}

Recall that we cannot compute $\vec{\phi}_{(\vec{x})}$ but can now 
exploit the kernel trick (cf. Eq.\ref{eq:trick}) by substituing 
$ K_{\alpha \beta} $ for
$
\vec{\phi}^{\top}_{(\vec{x}^{(\alpha)})}
 \, 
  \vec{\phi}_{(\vec{x}^{(\beta)})}
$

Eq.\ref{eq:eig2} becomes:
\begin{equation} \label{eq:eig3}
\frac{1}{p} \sum_{\alpha=1}^{p} \sum^{p}_{\beta=1} 
a^{(\beta)}
\vec{\phi}_{(\vec{x}^{(\alpha)})} K_{\alpha \beta}
 = \lambda 
\sum^{p}_{\beta=1} a^{(\beta)} \vec{\phi}_{(\vec{x}^{(\beta)})}
\end{equation}

We will now proceed with reformulating the above until we no longer have any $\phi$'s.

We left-multiply Eq.\ref{eq:eig3} with $\left(\vec \phi^{(\gamma)}\right)^\top$, where $\gamma = 1, \ldots, p$.
 We can pull $\left(\vec \phi^{(\gamma)}\right)^\top$ directly into the sum on the left and the sum on the right:

\begin{equation} \label{eq:eig4}
\frac{1}{p} \sum_{\alpha=1}^{p} \sum^{p}_{\beta=1} 
a^{(\beta)}
\underbrace{
\left(\vec \phi^{(\gamma)}\right)^\top
\vec{\phi}_{(\vec{x}^{(\alpha)})} 
}_{=K_{\gamma \alpha}}
K_{\alpha \beta}
 = \lambda 
\sum^{p}_{\beta=1} a^{(\beta)} 
\underbrace{
\left(\vec \phi^{(\gamma)}\right)^\top \vec{\phi}_{(\vec{x}^{(\beta)})}
}_{=K_{\gamma \beta}}
\end{equation}

\newpage

Eq.\ref{eq:eig4} without the clutter:

\begin{equation} \label{eq:eigK}
\frac{1}{p} \sum_{\alpha=1}^{p} \sum^{p}_{\beta=1} 
a^{(\beta)}
K_{\gamma \alpha}
K_{\alpha \beta}
 = \lambda 
\sum^{p}_{\beta=1} a^{(\beta)} 
K_{\gamma \beta} \quad \forall \gamma
\end{equation}

Since we want to compute this for all training samples $\gamma$, 
we can reduce the clutter even more by using matrix notation. 
Specifically, by using the \emph{kernel matrix} $\vec K=\{K_{\alpha\beta}\}$, where \\
$
K_{\alpha \beta} = 
k(\vec x^{(\alpha)}, \vec x^{(\beta)}) = 
\vec{\phi}_{(\vec{x}^{(\alpha)})}^\top 
		\vec{\phi}_{(\vec{x}^{(\beta)})}
$ 

We end up with this formulation of the eigenvalue problem:

\begin{equation*}
	\vec{K}^2 \vec{a} = p \lambda \vec{K} \mspace{2mu} \vec{a}
\end{equation*}

$\vec K$ appears on both sides. All the solutions that are of interest remain represented in 
the following simpler eigenvalue problem, which we refer to as the \emph{transformed eigenvalue problem}:
\begin{equation}
\label{eq:eigsimple1}
	\vec{K} \, \vec{a} = p \lambda \mspace{2mu} \vec{a}
\end{equation}

We can interpret $\vec a$ as the \emph{eigenvector} of $\vec K$

By omitting the constant $p$, we can rely on finding solutions for $\lambda$ that absorb it:

\begin{equation}
\label{eq:eigsimple2}
	\vec{K} \, \vec{a} = \lambda \mspace{2mu} \vec{a}
\end{equation}

All we've been doing so far is reformulate the eigenvalue problem such that we end up 
with a formulation that only contains terms of the inner product kernel.\\
Why was all this necesary? Because (1) we want to enable PCA to find non-linear correlations and (2) we don't have access to $\vec \phi_{(\vec x)}$.

Now that we've solved the eigenvalue problem, we continue with the remaining steps for PCA.

\newpage 
\item Normalize the eigenvectors:

Before we can project anything onto the space spanned by the PCs $\widetilde{\vec a}_k$ where $k=1,\ldots,p$,
we need to ensure these vectors are normalized. 
$\widetilde {\vec a}_k$ is only used to indicate that the vector has not been normalized yet.

%$\widetilde {\vec a}_k$ can be normalized explicitly by:
%$$
%\vec a_k^{norm.} = \frac{1}{||\widetilde{\vec a}_k||} \cdot \widetilde{\vec a}_k = 
%\frac{1}{\sqrt{\left(\widetilde{\vec a}_k\right)^\top \widetilde{\vec a}_k}} \cdot \widetilde{\vec a}_k
%$$

%However, we want to demonstrate how to normalize $\widetilde {\vec a}_k$ in a way that can be more efficient than an explicit normalization.

Recalling Eq.\ref{eq:ephi} (we add the index $k$ to denote which eigenvector):
\begin{equation}
\label{eq:ephik}
\vec e_k = \sum^{p}_{\beta=1} a_k^{(\beta)} \vec{\phi}_{(\vec{x}^{(\beta)})},
\end{equation}

Again, we try to reformulate things such that we end up with the inner-product kernel rather than $\phi$. 
We left-multiply Eq.\ref{eq:ephik} with $\left(\vec e_k\right)^\top$:
\begin{align}
\vec e^{\top}_k \vec e_k &= \sum^{p}_{\alpha=1}  a_k^{(\alpha)} \vec{\phi}_{(\vec{x}^{(\alpha)})}^\top \sum^{p}_{\beta=1} a_k^{(\beta)} \vec{\phi}_{(\vec{x}^{(\beta)})} \\
&= \sum^{p}_{\alpha=1}  \sum^{p}_{\beta=1}  a_k^{(\beta)}  \underbrace{\vec{\phi}_{(\vec{x}^{(\alpha)})}^\top  \vec{\phi}_{(\vec{x}^{(\beta)})}} a_k^{(\alpha)} \\
&= \sum^{p}_{\alpha=1}  \sum^{p}_{\beta=1}  a_k^{(\beta)} \quad \; K_{\alpha\beta} \quad \; a_k^{(\alpha)} \\
&= \widetilde {\vec a}_k^\top \vec K \, \widetilde {\vec a}_k
\end{align}

And when we plug Eq.\ref{eq:eigsimple1} into the above:

\begin{equation}
\label{eq:eignorm}
\vec e^{\top}_k \vec e_k = 
\widetilde {\vec a}_k^\top 
\underbrace{p \lambda_k \, \widetilde {\vec a}_k}_{=\, \vec K \, \widetilde {\vec a}_k} 
= p \lambda_k \, \widetilde {\vec a}_k^\top \widetilde {\vec a}_k  \eqexcl 1
\end{equation}

Scaling $\widetilde {\vec a}_k$ by $\frac{1}{\sqrt{p \lambda_k}}$ yields 
a unit vector with the same direction as $\widetilde {\vec a}_k$ to satisfy Eq.\ref{eq:eignorm}.\\
With
\begin{equation}
\vec a_k^{norm.} := \frac{1}{\sqrt{p \lambda_k}} \widetilde {\vec a}_k,
\end{equation}
follows:
\begin{align}
\vec e^{\top}_k \vec e_k 
&= p \lambda_k \, \; \widetilde {\vec a}_k^\top \; \widetilde {\vec a}_k\\
&= p \lambda_k \, \left(\vec a_k^{\text{norm.}}\right)^\top \vec a_k^{\text{norm.}} \\
&= p \lambda_k \left(\frac{1}{\sqrt{p \lambda_k}} \widetilde {\vec a}_k\right)^\top \left(\frac{1}{\sqrt{p \lambda_k}} \widetilde {\vec a}_k\right)
= 1
\end{align}


\newpage

\item Sort the eigenvectors such that the corresponding eigenvalues are arranged in decreasing order. 


\item Projection:

In order to project some observation $\vec x$ into the PC space, we first map it into the non-linear space of $\phi$ 
and then project that into the space spanned by the PCs. 
By now we should expect that the transformation can only be performed via the kernel trick. 
We basically represent this sample $\vec x$ by its relation to the \emph{training data} 
(i.e. the $p$ observations that were used to compute the PCs).

We derive the projection for Kernel PCA by starting with the projection used in linear PCA (cf. slides 1.3 \#11):

The projection for linear PCA, specifically the component of $\vec x$ in the direction of the $k$-th PC is:
\begin{equation}
\label{eq:projlin}
u_k(\vec x) = \vec e_k^\top \vec x
\end{equation}

We substitute $\vec \phi_{(\vec x)}$ for $\vec x$ and plug Eq.\ref{eq:ephi} into Eq.\ref{eq:projlin}:

\begin{align}
\label{eq:projk1}
u_k(\vec \phi_{(\vec x)}) &= \sum^{p}_{\beta=1} a^{(\beta)} 
\underbrace{
\vec{\phi}^\top_{(\vec{x}^{(\beta)})} \vec \phi_{(\vec x)}
}_{\substack{
\text{recognize the familiar}\\
\text{scalar product?}
\\ =k(\vec x^{(\beta)}, \vec x) = K_{\beta,\vec x}}}\\
&= \sum_{\beta=1}^{p} a_k^{(\beta)} K_{\beta, \vec x}
\end{align}

Note that $\vec x$ can be a sample that was used in computing the PCs or a completly new ``test'' point.

\item Reconstruction:

Since we never had the transformation $\phi$ to begin with. 
It is not psssible to simply project a sample from PC space back into the original $N$-dimensional input space. 
Algorithms exist that approximate a ``pre-image'' of some new observation.
\end{enumerate}


\section{linear PCA: Recap}
\begin{itemize}
\item Requires centering the data.
\item Eigenvalue problem: $\vec C\,\vec e = \lambda \vec e$
\item \underline{limited to linear correlations}
\end{itemize}

Kernel PCA for finding non-linear features.

Don't panic! Kernel PCA is essentially standard linear PCA applied to a non-linearly transformed version of the data.

\section{Non-linear transformations}

We use $N$ to denote the dimensionality of our input space and $p$ to denote the number of observations.\\
Let $\vec x^{(\alpha)} \in \R^N$ and $\alpha = 1, \ldots, p$.

The following mapping describes the non-linear transformation of each observation:
$$
\vec{\phi}: \vec{x} \mapsto \vec{\phi}_{(\vec{x})}
$$

An example of such a transformation - 2\textsuperscript{nd}-order monomials:
$$
\vec{\phi}_{(\vec{x})} = ( 
    1, \;
    \mathrm{x}_1, \;
    \mathrm{x}_2, \;
    \ldots \;
    \mathrm{x}_N, \;
    \mathrm{x}_1^2, \; 
    \mathrm{x}_1 \mathrm{x}_2, \;
    \mathrm{x}_2^2, \;
    \mathrm{x}_1 \mathrm{x}_3, \;
    \mathrm{x}_2 \mathrm{x}_3, \;
    \mathrm{x}_3^2, \; \ldots, \;
    \mathrm{x}_N^2
    )^\top
$$

We actually don't need to define this transformation.
All we need to know is that the dimensionality of $\vec{\phi}_{(\vec{x})}$ can be larger than $N$, possibly infinitely large.\\

\underline{The purpose of non-linear transformations:}

Two or more components in the original $\vec x$ (e.g. $x_1$, $x_2$) could have non-linear correlations (e.g. plotting those two components reveals a parabola). 
Expanding the dimensionality of $\vec x$ through the above mapping introduces new dimensions in which correlations between the components in $\vec \phi_{(\vec x)}$ become \emph{linear}.

\textbf{Caveat}:\\
Directly applying this transformation on a single observation is not applicable. 
We might never find a transformation that causes all non-linear correlations within $\vec x$ to become linear.

We turn to the ``kernel trick'' to solve this problem.

\newpage

\section{The kernel trick}

Representing a non-linear transformation using inner products of the data:
\begin{equation}
 \label{eq:trick}
      \vec{\phi}_{(\vec{x})}^\top 
		\vec{\phi}_{(\vec{x}')} = 
      k(\vec{x}, \vec{x}')
\end{equation}
    
where $k(\vec{x}, \vec{x}')$ is a kernel function applied 
to any two pairs of observations.

Applying the kernel function to \emph{each} pair in our dataset; \\
$\vec x^{(\alpha)}$ and $\vec{x}^{(\beta)}$ 
with $\alpha, \beta = 1, \ldots, p$ yields the scalar $K_{\alpha \beta}$. 
Storing all scalars $K_{\alpha \beta} \; \forall (\alpha,\beta)$ yields 
the un-normalized kernel matrix $\widetilde {\vec K}=\{K_{\alpha \beta}\}$:

$$
\widetilde {\vec K} = 
\rmat{
K_{11} & K_{12} & \ldots & K_{1p} \\
K_{21} & K_{12} & \ldots & K_{2p} \\
\vdots & & \ddots\\
K_{p1} & & & K_{pp}
}
$$

$\vec K$ (without the ``\textasciitilde'') denotes the normalized or ``centered'' kernel matrix.

\question{What is the dimensionality of $\widetilde {\vec K}$?}

\question{Is $K_{\alpha \beta}$ sensitive to translation and rotation of the data?}

- No. $K_{\alpha \beta}$ is the pairwise relation between two observations. 
Rotating the data or translating it will result in the same $K_{\alpha \beta}$. 
However, scaling the data while keeping the kernel function fixed would produce a different $K_{\alpha \beta}$.

\newpage 

\section{Kernel PCA}

We apply standard linear PCA on the \emph{transformed} version of the data
$
\left\{
\vec{\phi}_{(\vec{x}^{(\alpha)})}
\right\}_{\alpha=1}^{p}
$.
We will first assume we have $\vec{\phi}_{(\vec{x})}$ 
but we will eventually turn to $K_{\alpha \beta}$ 
which we can actually obtain.\\

\underline{Remark:}
A difference between these notes and the lecture slides is that 
the lecture slides employ ``identity'' as the mapping. 
This is why you don't see $\phi$ in the derivations of Kernel PCA in the slides but rather see $\vec x$ used directly.\\

\begin{enumerate}
\item PCA assumes its input is centered. 
$\frac{1}{p} \sum^{p}_{\alpha=1} \vec{\phi}_{(\vec{x}^{(\alpha)})}$\\
Centering $\vec X$ does not guranatee it stays centered after transformation.
Therefore, there is no need to center $\vec X$ beforehand.

\item Compute the covariance matrix $\vec C_{\phi}$ for $\vec{\phi}_{(\vec{x})}$:


\begin{equation} \label{eq:cov}
\vec C_{\phi} = \frac{1}{p} \sum_{\alpha=1}^{p} \vec{\phi}_{(\vec{x}^{(\alpha)})} \vec{\phi}^{\top}_{(\vec{x}^{(\alpha)})}
\end{equation}

\item Solve the eigenvalue problem:

\begin{equation} \label{eq:eig}
\vec C_{\phi} \, \vec e = \lambda \vec e
\end{equation}

Each eigenvector $\vec e_i$ with corresponding $\lambda_i \ne 0$ lies in the span of 
$
\left\{
\vec{\phi}_{(\vec{x}^{(\alpha)})}
\right\}_{\alpha=1}^{p}.
$

Consequently, there exists a set of coefficients (i.e. a coefficient for each transformed observation)
$
\left\{
a^{(\alpha)}
\right\}_{\alpha=1}^{p}
$, which satisfies the following:

\begin{equation}
\label{eq:ephi}
\vec e = \sum^{p}_{\beta=1} a^{(\beta)} \vec{\phi}_{(\vec{x}^{(\beta)})}
\end{equation}

Eq.\ref{eq:ephi} tells us that we can describe $\vec e$ in terms of the transformed observations (a weighted summation of $\phi$'s).
 The use of the index $\beta$ is only to avoid collisions with $\alpha$ later.

Substituting Eq.\ref{eq:cov} and Eq.\ref{eq:ephi} into the eignevalue problem Eq.\ref{eq:eig}:

\begin{equation*}
\underbrace{\frac{1}{p} \sum_{\alpha=1}^{p} \vec{\phi}_{(\vec{x}^{(\alpha)})} \vec{\phi}^{\top}_{(\vec{x}^{(\alpha)})} 
}_{=\,\vec C_{\phi}}
 \, 
\underbrace{\sum^{p}_{\beta=1} a^{(\beta)} \vec{\phi}_{(\vec{x}^{(\beta)})}}_{=\,\vec e}
 = \lambda \;\,
\underbrace{\sum^{p}_{\beta=1} a^{(\beta)} \vec{\phi}_{(\vec{x}^{(\beta)})}}_{=\,\vec e}
\end{equation*}

After rearranging the terms we get:
\begin{equation} \label{eq:eig2}
\frac{1}{p} \sum_{\alpha=1}^{p} \sum^{p}_{\beta=1} 
a^{(\beta)} \vec{\phi}_{(\vec{x}^{(\alpha)})}
\underbrace{
 \vec{\phi}^{\top}_{(\vec{x}^{(\alpha)})} \,  \vec{\phi}_{(\vec{x}^{(\beta)})}
}_{\substack{\text{scalar product}\\ = K_{\alpha\beta}}}
 = \lambda 
\sum^{p}_{\beta=1} a^{(\beta)} \vec{\phi}_{(\vec{x}^{(\beta)})}
\end{equation}

Recall that we cannot compute $\vec{\phi}_{(\vec{x})}$ but can now 
exploit the kernel trick (cf. Eq.\ref{eq:trick}) by substituing 
$ K_{\alpha \beta} $ for
$
\vec{\phi}^{\top}_{(\vec{x}^{(\alpha)})}
 \, 
  \vec{\phi}_{(\vec{x}^{(\beta)})}
$

Eq.\ref{eq:eig2} becomes:
\begin{equation} \label{eq:eig3}
\frac{1}{p} \sum_{\alpha=1}^{p} \sum^{p}_{\beta=1} 
a^{(\beta)}
\vec{\phi}_{(\vec{x}^{(\alpha)})} K_{\alpha \beta}
 = \lambda 
\sum^{p}_{\beta=1} a^{(\beta)} \vec{\phi}_{(\vec{x}^{(\beta)})}
\end{equation}

We will now proceed with reformulating the above until we no longer have any $\phi$'s.

We left-multiply Eq.\ref{eq:eig3} with $\left(\vec \phi^{(\gamma)}\right)^\top$, where $\gamma = 1, \ldots, p$.
 We can pull $\left(\vec \phi^{(\gamma)}\right)^\top$ directly into the sum on the left and the sum on the right:

\begin{equation} \label{eq:eig4}
\frac{1}{p} \sum_{\alpha=1}^{p} \sum^{p}_{\beta=1} 
a^{(\beta)}
\underbrace{
\left(\vec \phi^{(\gamma)}\right)^\top
\vec{\phi}_{(\vec{x}^{(\alpha)})} 
}_{=K_{\gamma \alpha}}
K_{\alpha \beta}
 = \lambda 
\sum^{p}_{\beta=1} a^{(\beta)} 
\underbrace{
\left(\vec \phi^{(\gamma)}\right)^\top \vec{\phi}_{(\vec{x}^{(\beta)})}
}_{=K_{\gamma \beta}}
\end{equation}

\newpage

Eq.\ref{eq:eig4} without the clutter:

\begin{equation} \label{eq:eigK}
\frac{1}{p} \sum_{\alpha=1}^{p} \sum^{p}_{\beta=1} 
a^{(\beta)}
K_{\gamma \alpha}
K_{\alpha \beta}
 = \lambda 
\sum^{p}_{\beta=1} a^{(\beta)} 
K_{\gamma \beta} \quad \forall \gamma
\end{equation}

Since we want to compute this for all training samples $\gamma$, 
we can reduce the clutter even more by using matrix notation. 
Specifically, by using the \emph{kernel matrix} $\vec K=\{K_{\alpha\beta}\}$, where \\
$
K_{\alpha \beta} = 
k(\vec x^{(\alpha)}, \vec x^{(\beta)}) = 
\vec{\phi}_{(\vec{x}^{(\alpha)})}^\top 
		\vec{\phi}_{(\vec{x}^{(\beta)})}
$ 

We end up with this formulation of the eigenvalue problem:

\begin{equation*}
	\vec{K}^2 \vec{a} = p \lambda \vec{K} \mspace{2mu} \vec{a}
\end{equation*}

$\vec K$ appears on both sides. All the solutions that are of interest remain represented in 
the following simpler eigenvalue problem, which we refer to as the \emph{transformed eigenvalue problem}:
\begin{equation}
\label{eq:eigsimple1}
	\vec{K} \, \vec{a} = p \lambda \mspace{2mu} \vec{a}
\end{equation}

We can interpret $\vec a$ as the \emph{eigenvector} of $\vec K$

By omitting the constant $p$, we can rely on finding solutions for $\lambda$ that absorb it:

\begin{equation}
\label{eq:eigsimple2}
	\vec{K} \, \vec{a} = \lambda \mspace{2mu} \vec{a}
\end{equation}

All we've been doing so far is reformulate the eigenvalue problem such that we end up 
with a formulation that only contains terms of the inner product kernel.\\
Why was all this necesary? Because (1) we want to enable PCA to find non-linear correlations and (2) we don't have access to $\vec \phi_{(\vec x)}$.

Now that we've solved the eigenvalue problem, we continue with the remaining steps for PCA.

\newpage 
\item Normalize the eigenvectors:

Before we can project anything onto the space spanned by the PCs $\widetilde{\vec a}_k$ where $k=1,\ldots,p$,
we need to ensure these vectors are normalized. 
$\widetilde {\vec a}_k$ is only used to indicate that the vector has not been normalized yet.

%$\widetilde {\vec a}_k$ can be normalized explicitly by:
%$$
%\vec a_k^{norm.} = \frac{1}{||\widetilde{\vec a}_k||} \cdot \widetilde{\vec a}_k = 
%\frac{1}{\sqrt{\left(\widetilde{\vec a}_k\right)^\top \widetilde{\vec a}_k}} \cdot \widetilde{\vec a}_k
%$$

%However, we want to demonstrate how to normalize $\widetilde {\vec a}_k$ in a way that can be more efficient than an explicit normalization.

Recalling Eq.\ref{eq:ephi} (we add the index $k$ to denote which eigenvector):
\begin{equation}
\label{eq:ephik}
\vec e_k = \sum^{p}_{\beta=1} a_k^{(\beta)} \vec{\phi}_{(\vec{x}^{(\beta)})},
\end{equation}

Again, we try to reformulate things such that we end up with the inner-product kernel rather than $\phi$. 
We left-multiply Eq.\ref{eq:ephik} with $\left(\vec e_k\right)^\top$:
\begin{align}
\vec e^{\top}_k \vec e_k &= \sum^{p}_{\alpha=1}  a_k^{(\alpha)} \vec{\phi}_{(\vec{x}^{(\alpha)})}^\top \sum^{p}_{\beta=1} a_k^{(\beta)} \vec{\phi}_{(\vec{x}^{(\beta)})} \\
&= \sum^{p}_{\alpha=1}  \sum^{p}_{\beta=1}  a_k^{(\beta)}  \underbrace{\vec{\phi}_{(\vec{x}^{(\alpha)})}^\top  \vec{\phi}_{(\vec{x}^{(\beta)})}} a_k^{(\alpha)} \\
&= \sum^{p}_{\alpha=1}  \sum^{p}_{\beta=1}  a_k^{(\beta)} \quad \; K_{\alpha\beta} \quad \; a_k^{(\alpha)} \\
&= \widetilde {\vec a}_k^\top \vec K \, \widetilde {\vec a}_k
\end{align}

And when we plug Eq.\ref{eq:eigsimple1} into the above:

\begin{equation}
\label{eq:eignorm}
\vec e^{\top}_k \vec e_k = 
\widetilde {\vec a}_k^\top 
\underbrace{p \lambda_k \, \widetilde {\vec a}_k}_{=\, \vec K \, \widetilde {\vec a}_k} 
= p \lambda_k \, \widetilde {\vec a}_k^\top \widetilde {\vec a}_k  \eqexcl 1
\end{equation}

Scaling $\widetilde {\vec a}_k$ by $\frac{1}{\sqrt{p \lambda_k}}$ yields 
a unit vector with the same direction as $\widetilde {\vec a}_k$ to satisfy Eq.\ref{eq:eignorm}.\\
With
\begin{equation}
\vec a_k^{norm.} := \frac{1}{\sqrt{p \lambda_k}} \widetilde {\vec a}_k,
\end{equation}
follows:
\begin{align}
\vec e^{\top}_k \vec e_k 
&= p \lambda_k \, \; \widetilde {\vec a}_k^\top \; \widetilde {\vec a}_k\\
&= p \lambda_k \, \left(\vec a_k^{\text{norm.}}\right)^\top \vec a_k^{\text{norm.}} \\
&= p \lambda_k \left(\frac{1}{\sqrt{p \lambda_k}} \widetilde {\vec a}_k\right)^\top \left(\frac{1}{\sqrt{p \lambda_k}} \widetilde {\vec a}_k\right)
= 1
\end{align}


\newpage

\item Sort the eigenvectors such that the corresponding eigenvalues are arranged in decreasing order. 


\item Projection:

In order to project some observation $\vec x$ into the PC space, we first map it into the non-linear space of $\phi$ 
and then project that into the space spanned by the PCs. 
By now we should expect that the transformation can only be performed via the kernel trick. 
We basically represent this sample $\vec x$ by its relation to the \emph{training data} 
(i.e. the $p$ observations that were used to compute the PCs).

We derive the projection for Kernel PCA by starting with the projection used in linear PCA (cf. slides 1.3 \#11):

The projection for linear PCA, specifically the component of $\vec x$ in the direction of the $k$-th PC is:
\begin{equation}
\label{eq:projlin}
u_k(\vec x) = \vec e_k^\top \vec x
\end{equation}

We substitute $\vec \phi_{(\vec x)}$ for $\vec x$ and plug Eq.\ref{eq:ephi} into Eq.\ref{eq:projlin}:

\begin{align}
\label{eq:projk1}
u_k(\vec \phi_{(\vec x)}) &= \sum^{p}_{\beta=1} a^{(\beta)} 
\underbrace{
\vec{\phi}^\top_{(\vec{x}^{(\beta)})} \vec \phi_{(\vec x)}
}_{\substack{
\text{recognize the familiar}\\
\text{scalar product?}
\\ =k(\vec x^{(\beta)}, \vec x) = K_{\beta,\vec x}}}\\
&= \sum_{\beta=1}^{p} a_k^{(\beta)} K_{\beta, \vec x}
\end{align}

Note that $\vec x$ can be a sample that was used in computing the PCs or a completly new ``test'' point.

\item Reconstruction:

Since we never had the transformation $\phi$ to begin with. 
It is not psssible to simply project a sample from PC space back into the original $N$-dimensional input space. 
Algorithms exist that approximate a ``pre-image'' of some new observation.
\end{enumerate}

\subsection{Centering the kernel matrix}

PCA operates on centered data. This is common for all forms  of PCA, 
whether it is linear PCA, online PCA or Kernel PCA.
In Kernel PCA, the $\vec X$ is transformed into a higher dimensional space by applying the kernel trick.
Applying it to all $p$ training samples yields the kernel matrix, which is used in solving the transformed eigenvalue problem.

All of the above assumed that we have successfully centered the kernel matrix. We now look at how this is done.

\question{Do I need to center $\vec X$ before computing $ K_{\alpha \beta}$?}

- No, Centering $\vec X$ before computing $K_{\alpha \beta}$ does not guarantee the kernel matrix to be centered. You only end up with $\vec{\widetilde K}$. 
It is simply irrelevant whether the original data $\vec X$ is centered or not. 
Therefore, we need to center the kernel matrix before solving the transformed eigenvalue problem.\\

We use $\vec{\widetilde K}$ to denote the \emph{un-normalized} kernel matrix and $\vec K$ 
to denote the kernel matrix \emph{after} it has been centered.

\underline{Centering $\vec{\widetilde K}$:}

The centering is built on the kernel trick as in Eq.\ref{eq:trick}, except that we compute the inner product using centered $\phi_{(\vec x)}$:

\begin{equation*}
	\underbrace{ \vec{\phi}_{\big( \vec{x}^{(\alpha)} \big)} }_{
		\substack{ 	\text{``centered''} \\
				\text{feature vectors}} }
	= \widetilde{\vec{\phi}}_{\big( \vec{x}^{(\alpha)} \big)}
		- \frac{1}{p} \sum\limits_{\gamma = 1}^p 
		\underbrace{ \widetilde{\vec{\phi}}_{\big( \vec{x}^{(\gamma)} 
				\big)} }_{
			\substack{ 	\text{uncentered} \\
					\text{feature vectors}} }
					\qquad \forall \alpha
\end{equation*}

\begin{equation*}
	K_{\alpha \beta} = \underbrace{\widetilde{K}_{\alpha \beta}}_{ _{= \, k \left ( \vec{x}^{(\alpha)},
			\vec{x}^{(\beta)} \right )}}
		- \;\underbrace{\frac{1}{p} \sum\limits_{\delta = 1}^p 
		\widetilde{K}_{\alpha \delta}}_\text{\scriptsize{row avg.}} 
		\; - \; \underbrace{\frac{1}{p} 
		\sum\limits_{\gamma = 1}^p 
		\widetilde{K}_{\gamma \beta}}_\text{\scriptsize{col. avg.}}
		\;+ \;\underbrace{\frac{1}{p^2} 
		\sum\limits_{\gamma, \delta = 1}^p 
		\widetilde{K}_{\gamma \delta}}_\text{\scriptsize{matrix avg.}}
\end{equation*}%

(cf. slides 1.3 \#16-\#17 on how we arrive at the above centering)

\subsection{Applying Kernel PCA:}

\question{How do I interpret the role of PCs?}


(cf. 1.3 \#28)

\subsection{A note on implementation:}

\question{Should you solve the transformed eigenvalue problem using \emph{eigenvalue decomposition} or \emph{SVD}?}

- \emph{eigenvalue decomposition} is the only option for Kernel PCA. \emph{SVD} is simply not applicable since we don't have access to $\vec \phi_((\vec x))$

The kernel function is symmetric. $k(\vec x^{(\alpha)}, \vec x^{(\beta)}) = k(\vec x^{(\beta)}, \vec x^{(\alpha)})$. One can exploit this by reducing how many times the kernel function is actually applied while traversing the training samples when computing $\widetilde {\vec{K}}$.

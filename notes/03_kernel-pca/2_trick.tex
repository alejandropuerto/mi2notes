\section{The kernel trick}

\begin{frame}\frametitle{\secname}

Representing a non-linear transformation using inner products of the data:
\begin{equation}
 \label{eq:trick}
      \vec{\phi}_{(\vec{x})}^\top 
		\vec{\phi}_{(\vec{x}')} = 
      k(\vec{x}, \vec{x}')
\end{equation}
    
where $k(\vec{x}, \vec{x}')$ is a kernel function applied 
to any two observations.

\end{frame}

\subsection{The Kernel matrix}

\begin{frame}\frametitle{\subsecname}

Applying the kernel function to \emph{each} pair in our dataset; \\
$\vec x^{(\alpha)}$ and $\vec{x}^{(\beta)}$ 
with $\alpha, \beta = 1, \ldots, p$ yields the scalar $K_{\alpha \beta}$. 
Storing all scalars $K_{\alpha \beta} \; \forall (\alpha,\beta)$ yields 
the un-normalized kernel matrix $\widetilde {\vec K}=\{K_{\alpha \beta}\}$:

\begin{equation}
\widetilde {\vec K} = 
\rmat{
K_{11} & K_{12} & \ldots & K_{1p} \\
K_{21} & K_{12} & \ldots & K_{2p} \\
\vdots & & \ddots\\
K_{p1} & & & K_{pp}
}
\end{equation}

$\vec K$ (without the ``\textasciitilde'') denotes the normalized or ``centered'' kernel matrix. \notesonly{cf. \sectionref{sec:centerkernel} why and how to center the Kernel matrix.}

\question{What is the dimensionality of $\widetilde {\vec K}$?}

\end{frame}

\subsubsection{Properties of the Kernel matrix}

\begin{frame}\frametitle{\subsubsecname}

\begin{block}{From Mercer's theorem}
Every positive semidefinite kernel k corresponds to a scalar product in
some metric feature space.
\end{block}

\end{frame}

\begin{frame}\frametitle{The RBF Kernel}

The Radial Basis function (RBF) 

\question{Is $K_{\alpha \beta}$ sensitive to translation and rotation of the data?}

- No. $K_{\alpha \beta}$ is the pairwise relation between two observations. 
Rotating the data or translating it will result in the same $K_{\alpha \beta}$. 
However, scaling the data while keeping the kernel function fixed would produce a different $K_{\alpha \beta}$.

\end{frame}

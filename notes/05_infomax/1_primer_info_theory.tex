\subsection{Entropy}

Entropy is a measure of uncertainty. It measures the average amount of information conveyed by a message.

\textbf{Definition:} 
The entropy $H(X)$ of a discrete random variable $X$ is defined by:

\begin{equation}
\label{eq:entropy}
H(X) := - \sum_{x \in \mathcal X} p_X(x)\,\log p_X(x)
\end{equation}

Imagine flipping a biased coin which reveals tails most of the time. 
We will no longer be ``surprised'' if we flip the coin again and see it show tails again. 
There is no information gained from flipping the coin another time. There is no uncertainty left.

However if it were an unbiased coin, we would be an uncertain and ``surprised'' by the outcome every time.

We do not have an equivalent measure for continuous random variables. 
However we can resort to the \emph{differential entropy} $h(X)$ of a \textbf{continuous} variable $X$
\footnote{if interested, see Haykin Ch 10.2 for the relation betweenn $H(X)$ and $h(X)$.}:

\begin{equation}
\label{eq:entropydiff}
h(X) := - \int_{-\infty}^{\infty} p_X(x)\,\log p_X(x) \, dx 
= - \E \lbrack \log p_X(x) \rbrack
\end{equation}

%\exercise{Joint Entropy}

%\textbf{Definition:} 
%The joint entropy of $H(X,Y)$ of a pair of discrete random variables $(X,Y)$ with a joint pdf $p_{X,Y}(x,y)$ is defined as:

%\begin{equation}
%\label{eq:entropyjoint}
%H(X,Y) = - \sum_{x \in \mathcal X} \sum_{y \in \mathcal{Y}} p_{X,Y}(x,y)\,\log p_{X,Y}(x,y)
%\end{equation}

\subsection{Conditional Entropy}

Conditional entropy measures entropy of a random variable after one has observed the event of another variable.
In the case of a system with input $X$ and output $Y$.

\textbf{Definition:} 
The conditional entropy of $H(Y|X)$ of a pair of discrete random variables $(X,Y)$ with a joint pdf $p_{X,Y}(x,y)$ is defined as:

\begin{align}
\label{eq:entropycond}
H(Y|X) 
&= \sum_{x \in \mathcal{X}} p_X(x) H(Y|X=x) \\
&= -\sum_{x \in \mathcal{X}} p_X(x) \sum_{y \in \mathcal{Y}} p(y|x)\,\log p(y|x) \\
&= -\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p_X(x) p(y|x)\,\log p(y|x) \\
&= -\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p_{X,Y}(x,y)\,\log p(y|x) \\
&= -\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p_{X,Y}(x,y)\,\log \frac{p_{X,Y}(x,y)}{p_X(x)} \\
&= -\E_{p_{X,Y}(x,y)} \lbrack \, \log p(Y|X) \, \rbrack \\
\end{align}

$H(Y|X)$ represents the amount of uncertainty remaining about a system's output $Y$ after the 
system's input $X$ has been observed. 
\emph{
$H(Y|X)$ is whatever entropy the output [$Y$] has that did not come from the input [$X$]
}
\footnote{
Bell, A. J., \& Sejnowski, T. J. (1995). An information-maximization approach to blind separation and blind deconvolution. Neural computation, 7(6), 1129-1159.
}.

For the continuous case, we measure the conditional differential entropy $h(Y|X)$ which is defined as:

\begin{equation}
h(Y|X) = -\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} p_{X,Y}(x,y)\,\log  p_{Y}(y|x) \, dx \, dy
\label{eq:diffentropycondcontinuous}
\end{equation}

\subsection{Relative Entropy or Kullback-Leibler (KL) divergence}

A measure for how a much one probability distribution differs from a reference distribution. 
It does not qualify as a distance metric, as it is not symmetric, but it still satisfies other useful metric properties (e.g. non-negative).

\textbf{Definition:} 
The relative entropy or Kullback-Leibler divergence between two pdfs $p(x)$ and $q(x)$ is defined as

%\begin{align}
%\label{eq:kldiscrete}
%D_{KL}(p||q) 
%&= \sum{x \in } p_X(x) \log \frac{p_X(x)}{q(x)} \\
%&= -\E_{p_{X}(x)} \,\log \frac{p(X)}{q(X)}
%\end{align}

%for the discrete case. And

\begin{align}
\label{eq:klcont}
D_{KL}\lbrack\,p\, ||\, q\,\rbrack 
&= \int_{-\infty}^{\infty} p(x) \log \left( \frac{p(x)}{q(x)} \right) dx
\end{align}

for the continuous case.

\clearpage 
Let's start putting things into the context of ICA:

Recalling the factorization in \eqref{eq:facts} which is based on the assumption that such facotrization can be found by minimizing 
the KL-divergence between the joint distribution of the reconstructed sources and the product of their marginal distributions, specifically:

\begin{equation}
\label{eq:klmin}
	\dkl[P_{\vec{s}}(\widehat{\vec{s}}),\widehat{P}_{\vec{s}}(\widehat{\vec{s}})] = 
    \int d \, \widehat{\vec{s}} \; P_{\vec{s}}(\widehat{\vec{s}})
		\ln \frac{P_{\vec{s}}(\widehat{\vec{s}})}{
			\prod_{i = 1}^N \widehat{P}_{s_i}(\widehat{s}_i) }
		\eqexcl \min_{\vec{W}}
\end{equation}

In spite of this being an apparent approach for blind source separation, it requires a paramtereized density estimate which is computationally costly.

\subsection{Mutual Information}

The entropy $H(X)$ reprsents our uncertainty about the system's input $X$ 
and the conditional entropy $H(X|Y)$ represents such \textbf{after} observing its output $Y$.

Thus, the difference between them represents the uncertainty about $X$ \textbf{that is resolved} after observing $Y$. 
This is referred to as the mutual information $I(X,Y)$ between the random variables $X$ and $Y$:

%\begin{align}
%\label{eq:mutualdiscrete}
%I(X,Y) &= H(X) - H(X|Y) \\
%&= \sum{x \in } \sum{y \in } p_{X,Y}(x,y) \, \log \left(\frac{p(x,y)}{p_X(x) p_Y(y)}\right)
%&= H(Y) - H(Y|X) = I(Y,X)
%\end{align}

%for the discrete case. And

\begin{align}
\label{eq:mutualcont1}
I(X,Y) 
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} p_{X,Y}(x,y) \, \log \left(\frac{p_X(x|y)}{p_X(x)}\right) dx \, dy \\
\end{align}

and with $p_{X,Y} = p_X(x)\,p_Y(y)$ we get:

\begin{align}
\label{eq:mutualcont2}
I(X,Y)
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} p_{X,Y}(x,y) \, \log \left(\frac{p_{X,Y}(x,y)}{p_X(x) p_Y(y)}\right) dx \, dy
\end{align}

for the continuous case in addition to the following properties:
\begin{align}
\label{eq:mutualcontprops}
I(X,Y) &= h(X) - h(X|Y)\\
&= h(Y) - h(Y|X)\\
%&= h(X) + h(Y) - h(X,Y) 
&= I(Y,X) \ge 0
\end{align}

Note that 
$I(X_1,X_2)=0$ if the the random variables are statistically independent. But we won't make much use of this special case equality in ICA.


\newpage

\subsection{Relationship between the KL-Divergence and Mutual Information}

Not really another approach. This is just to show how to arrive at the same expression as in the previous approach.

Recall the definition of the mutual information between input and output variables from \eqref{eq:mutualcont2}:

\begin{equation*}
I(X,Y) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} p_{X,Y}(x,y) \, \log \left(\frac{p_{X,Y}(x,y)}{p_X(x) p_Y(y)}\right) dx \, dy
\end{equation*}

and the definition for the KL divergence from \eqref{eq:klcont}:

\begin{align*}
D_{KL}\lbrack\,p\, ||\, q\,\rbrack 
&= \int_{-\infty}^{\infty} p(x) \log \left( \frac{p(x)}{q(x)} \right) dx.
\end{align*}

We can deduce that:
$$
I(X,Y) = D_{KL} \lbrack \, p_{X,Y}(x,y) \, || \, p_X(x) p_Y(y) \, \rbrack
$$

Although $I(X,Y) = I(Y,X)$ the same cannot be said about switching the arguments of the KL-divergence.

$
D_{KL} \lbrack \, p_{X,Y}(x,y) \, || \, p_X(x) p_Y(y) \, \rbrack \ne 
D_{KL} \lbrack \, p_X(x) p_Y(y) \, || \, p_{X,Y}(x,y)\, \rbrack
$

They are only equal when both of them are equal to zero, which is satisfied iff $p_{X,Y}(x,y) = p_X(x) p_Y(y)$.

This tells us that the mutual information $I(X,Y)$ is equivalent to the KL-divergence between the joint distribution $p_{X,Y}(x,y)$ and 
the product of the pdfs $p_X(x)$ and $p_Y(y)$. A special case of this is what we saw in the first approach, namely the KL-divergence 
between the pdf of a random vector $\widehat {\vec s}$ of length $N$ and the marginal probability density functions for $N$ elements 
$\widehat{s_i}$\footnote{A much smoother walkthrough for this can be found in 
Haykin Ch. 10.5}

\clearpage

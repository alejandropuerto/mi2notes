
\section{The ICA problem:}

Let $\vec s = (s_1, s_2,...,s_N)^\top$ denote the concatenation of $N$ \underline{independent sources} 
and $\vec x \in \R^N$ describe our observations. $\vec x$ relates to $\vec s$ through a 
\emph{linear transformation} $\vec A$:

\begin{equation}
\label{eq:ica}
\vec x = \vec A \, \vec s
\end{equation}

Again with individual elements and with $N=2$:
\begin{equation}
 \left( \begin{array}{ll}
			x_1 \\ x_2
		\end{array} \right)
        = \left( \begin{array}{ll}
			a_{11} & a_{12} \\ a_{21} & a_{22}
		\end{array} \right) \cdot \left( \begin{array}{ll}
			s_1 \\ s_2
		\end{array} \right)
	= \left( \begin{array}{l}
		a_{11} s_1 + a_{12} s_2 \\ a_{21} s_1 + a_{22} s_2
	\end{array} \right)
\end{equation}

We refer to $\vec A$ as the \emph{mixing matrix} and \eqref{eq:ica} as the \emph{ICA problem}, 
which is recovering $\vec s$ from only observing $\vec x$.

The ICA problem can be solved by finding the \emph{unmixing matrix} $\vec W$ such that 
\begin{equation}
\vec s = \vec W \cdot \vec x = \vec A^{-1} \cdot \vec x
\end{equation}

But since we don't have the original mixing matrix $\vec A$ we employ methods of ICA for finding an unmixing matrix $\vec W$ which yields an estimate of the sources, namely $\vec {\hat s}$. 
\begin{equation}
 \left( \begin{array}{ll}
			\hat s_1 \\ \hat s_2
		\end{array} \right)
        = \left( \begin{array}{ll}
			w_{11} & w_{12} \\ w_{21} & w_{22}
		\end{array} \right) \cdot \left( \begin{array}{ll}
			x_1 \\ x_2
		\end{array} \right)
	= \left( \begin{array}{l}
		w_{11} \hat s_1 + w_{12} \hat s_2 \\ w_{21} \hat s_1 + w_{22} \hat s_2
	\end{array} \right)
\end{equation}

We will discuss several methods for solving the ICA problem:

\begin{itemize}
\item Methods that maximize the \emph{mutual information} between the observations $\vec x$ and the estimated sources $\vec {\hat s}$ (e.g. Infomax)
\item Methods for maximizing the \emph{non-gaussianity} of $\vec {\hat s}$ (e.g. Kurtosis-based ICA, FastICA)
\end{itemize}

We first will look into some key concepts employed by the \emph{Infomax} method.

\clearpage

\underline{Outline for \emph{Infomax} ICA:}
\begin{itemize}
    \item Uncorrelatedness
    \item \emph{statistical independence}
    \item A primer on Information theory
    \begin{itemize}
        \item Entropy
        \item Conditional Entropy
        \item Relative Entropy or \emph{Kullback-Leibler (KL) divergence}
        \item Mutual Information
    \end{itemize}
    \item cost function
    \item ERM
    \item practical aspects
\end{itemize}

\subsection{Uncorrelatedness:}

Correlation measures the linear relationship between two random variables.
\begin{center}
\includegraphics[width=0.4\textwidth]{img/section2_fig3.pdf}
\end{center}

If no such correlation exists between two random scalar variables $x_1$ and $x_2$, i.e.

\begin{equation}
\label{eq:uncorr}
\mathrm{Cov}(x_1, x_2) = \E  \lbrack x_1  x_2 \rbrack - \E  \lbrack x_1 \rbrack \E \lbrack x_2 \rbrack  = 0
\end{equation}

\begin{equation}
\E  \lbrack x_1  x_2 \rbrack = \E  \lbrack x_1 \rbrack \E \lbrack x_2 \rbrack
\end{equation}

The two variables are \emph{uncorrelated}.

\subsection{Statistical independence:}

Independence is a much stronger property than decorrelatedness. 

Consider the following two random scalar variables $x$ and $y$:
\begin{center}
\includegraphics[width=0.4\textwidth]{img/uniform_fixed.png}
\end{center}

Knowing anything about $x$ reveals absolutely nothing about $y$, and vice-versa. This is because both variables are \emph{independent}. 
Independence requires that the joint probability $p_{X,Y}(x,y)$ factorizes into the product of 
the marginal probabilities $p_X(x)$ and  $p_Y(y)$:

\begin{equation}
\label{eq:statindep}
p_{X,Y}(x,y) = p_X(x) p_Y(y)
\end{equation}

From this follows:

\begin{equation}
\label{eq:statindepexp}
\E  \lbrack \, g(x) h(y) \, \rbrack = \E \lbrack g(x) \rbrack \, \E \lbrack h(y) \rbrack  \,,
\end{equation}

where $g(x)$ and $h(y)$ are absolutely integrable functions of $x$ and $y$, respectively. 
This can be shown with the use of \eqref{eq:statindep}:

\begin{align}
\E  \lbrack \, g(x) h(y) \, \rbrack &= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x) \, h(y) \, p_{X,Y}(x,y) \, dx \, dy \\
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x) \, h(y)  \, p_X(x) \, p_Y(y) \, dx \, dy \\
&= \int_{-\infty}^{\infty}  g(x) \, p_X(x) dx\, \int_{-\infty}^{\infty} h(y) \, p_Y(y) dy
\,.
\end{align}

If $x$ and $y$ are independent, they are also uncorrelated, but the same cannot always be said the other way around.

Example\footnote{Example obtained from (Hyv\"arinen, A., \& Oja, E. (2000). Independent component analysis: algorithms and applications. Neural networks, 13(4-5), 411-430.)}:

Assume $x$ and $y$ are discrete valued and follow a distribution that the pair are with probability 1/4 equal to any of the following
values: (0,1),(0,-1),(1,0),(-1,0). Then $x$ and $y$ are uncorrelated. This is simple to calculate:

$\E \lbrack x \rbrack = \frac{1}{4} \cdot (0 + 0 + 1 + (-1)) = 0$\\[2mm]
$\E \lbrack y \rbrack = \frac{1}{4} \cdot (1 + (-1) + 0 + 0) = 0$\\

On the other hand, if we calculate $\E  \lbrack \, g(x) h(y) \, \rbrack$ with $g(x)=x^2$ and $g(y)=y^2$:\\

$\E \lbrack x^2 \, y^2 \rbrack 
= \frac{1}{4} \cdot \lbrack (0\cdot1) + (0\cdot1) + (1\cdot0) + (1\cdot0) \rbrack = 0 
\ne \frac{1}{4} = \E \lbrack x^2 \rbrack \, \E \lbrack y^2 \rbrack \;,$

we see that the requirement for independence in \eqref{eq:statindepexp} is violated. The variables are therefore uncorrelated but not independent.

\clearpage

\question{How does statistical independence fit into ICA?}

- The solution to the ICA problem yields a distribution of the estimated source $\widehat{P}_{\vec s}(\widehat{\vec s})$ that is closest to the factorizing density:

\begin{equation}
\label{eq:facts}
\widehat{P}_{\vec s}(\widehat{\vec s}) = \prod_{i=1}^{N} \widehat{P}_{s_i}(\widehat{s_i})  \,.
\end{equation}

To clarify the notation here:\\
\begin{itemize}
\vspace{-5mm}
%\renewcommand\labelitemi{--}
\setlength\itemsep{0.1em}
\item $\vec s$ describes the independent sources which we do not have.
\item $\widehat{\vec s}$ are the reconstructed sources which we obtain via some unmixing matrix $\vec W$.
\item $\widehat{P}_{\vec s}(\widehat{\vec s})$ is our estimate of the pdf of $\vec s$ which we have approximated using $\widehat{\vec s}$. 
\end{itemize}

\subsection{Entropy}

Entropy is a measure of uncertainty. It measures the average amount of information conveyed by a message.

\textbf{Definition:} 
The entropy $H(X)$ of a discrete random variable $X$ is defined by:

\begin{equation}
\label{eq:entropy}
H(X) := - \sum_{x \in \mathcal X} p_X(x)\,\log p_X(x)
\end{equation}

Imagine flipping a biased coin which reveals tails most of the time. 
We will no longer be ``surprised'' if we flip the coin again and see it show tails again. 
There is no information gained from flipping the coin another time. There is no uncertainty left.

However if it were an unbiased coin, we would be an uncertain and ``surprised'' by the outcome every time.

We do not have an equivalent measure for continuous random variables. 
However we can resort to the \emph{differential entropy} $h(X)$ of a \textbf{continuous} variable $X$
\footnote{if interested, see Haykin Ch 10.2 for the relation betweenn $H(X)$ and $h(X)$.}:

\begin{equation}
\label{eq:entropydiff}
h(X) := - \int_{-\infty}^{\infty} p_X(x)\,\log p_X(x) \, dx 
= - \E \lbrack \log p_X(x) \rbrack
\end{equation}

%\exercise{Joint Entropy}

%\textbf{Definition:} 
%The joint entropy of $H(X,Y)$ of a pair of discrete random variables $(X,Y)$ with a joint pdf $p_{X,Y}(x,y)$ is defined as:

%\begin{equation}
%\label{eq:entropyjoint}
%H(X,Y) = - \sum_{x \in \mathcal X} \sum_{y \in \mathcal{Y}} p_{X,Y}(x,y)\,\log p_{X,Y}(x,y)
%\end{equation}

\subsection{Conditional Entropy}

Conditional entropy measures entropy of a random variable after one has observed the event of another variable.
In the case of a system with input $X$ and output $Y$.

\textbf{Definition:} 
The conditional entropy of $H(Y|X)$ of a pair of discrete random variables $(X,Y)$ with a joint pdf $p_{X,Y}(x,y)$ is defined as:

\begin{align}
\label{eq:entropycond}
H(Y|X) 
&= \sum_{x \in \mathcal{X}} p_X(x) H(Y|X=x) \\
&= -\sum_{x \in \mathcal{X}} p_X(x) \sum_{y \in \mathcal{Y}} p(y|x)\,\log p(y|x) \\
&= -\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p_X(x) p(y|x)\,\log p(y|x) \\
&= -\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p_{X,Y}(x,y)\,\log p(y|x) \\
&= -\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p_{X,Y}(x,y)\,\log \frac{p_{X,Y}(x,y)}{p_X(x)} \\
&= -\E_{p_{X,Y}(x,y)} \lbrack \, \log p(Y|X) \, \rbrack \\
\end{align}

$H(Y|X)$ represents the amount of uncertainty remaining about a system's output $Y$ after the 
system's input $X$ has been observed. 
\emph{
$H(Y|X)$ is whatever entropy the output [$Y$] has that did not come from the input [$X$]
}
\footnote{
Bell, A. J., \& Sejnowski, T. J. (1995). An information-maximization approach to blind separation and blind deconvolution. Neural computation, 7(6), 1129-1159.
}.

For the continuous case, we measure the conditional differential entropy $h(Y|X)$ which is defined as:

\begin{equation}
h(Y|X) = -\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} p_{X,Y}(x,y)\,\log  p_{Y}(y|x) \, dx \, dy
\label{eq:diffentropycondcontinuous}
\end{equation}

\subsection{Relative Entropy or Kullback-Leibler (KL) divergence}

A measure for how a much one probability distribution differs from a reference distribution. 
It does not qualify as a distance metric, as it is not symmetric, but it still satisfies other useful metric properties (e.g. non-negative).

\textbf{Definition:} 
The relative entropy or Kullback-Leibler divergence between two pdfs $p(x)$ and $q(x)$ is defined as

%\begin{align}
%\label{eq:kldiscrete}
%D_{KL}(p||q) 
%&= \sum{x \in } p_X(x) \log \frac{p_X(x)}{q(x)} \\
%&= -\E_{p_{X}(x)} \,\log \frac{p(X)}{q(X)}
%\end{align}

%for the discrete case. And

\begin{align}
\label{eq:klcont}
D_{KL}\lbrack\,p\, ||\, q\,\rbrack 
&= \int_{-\infty}^{\infty} p(x) \log \left( \frac{p(x)}{q(x)} \right) dx
\end{align}

for the continuous case.

\clearpage 
Let's start putting things into the context of ICA:

Recalling the factorization in \eqref{eq:facts} which is based on the assumption that such facotrization can be found by minimizing 
the KL-divergence between the joint distribution of the reconstructed sources and the product of their marginal distributions, specifically:

\begin{equation}
\label{eq:klmin}
	\dkl[P_{\vec{s}}(\widehat{\vec{s}}),\widehat{P}_{\vec{s}}(\widehat{\vec{s}})] = 
    \int d \, \widehat{\vec{s}} \; P_{\vec{s}}(\widehat{\vec{s}})
		\ln \frac{P_{\vec{s}}(\widehat{\vec{s}})}{
			\prod_{i = 1}^N \widehat{P}_{s_i}(\widehat{s}_i) }
		\eqexcl \min_{\vec{W}}
\end{equation}

In spite of this being an apparent approach for blind source separation, it requires a paramtereized density estimate which is computationally costly.

\subsection{Mutual Information}

The entropy $H(X)$ reprsents our uncertainty about the system's input $X$ 
and the conditional entropy $H(X|Y)$ represents such \textbf{after} observing its output $Y$.

Thus, the difference between them represents the uncertainty about $X$ \textbf{that is resolved} after observing $Y$. 
This is referred to as the mutual information $I(X,Y)$ between the random variables $X$ and $Y$:

%\begin{align}
%\label{eq:mutualdiscrete}
%I(X,Y) &= H(X) - H(X|Y) \\
%&= \sum{x \in } \sum{y \in } p_{X,Y}(x,y) \, \log \left(\frac{p(x,y)}{p_X(x) p_Y(y)}\right)
%&= H(Y) - H(Y|X) = I(Y,X)
%\end{align}

%for the discrete case. And

\begin{align}
\label{eq:mutualcont1}
I(X,Y) 
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} p_{X,Y}(x,y) \, \log \left(\frac{p_X(x|y)}{p_X(x)}\right) dx \, dy \\
\end{align}

and with $p_{X,Y} = p_X(x)\,p_Y(y)$ we get:

\begin{align}
\label{eq:mutualcont2}
I(X,Y)
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} p_{X,Y}(x,y) \, \log \left(\frac{p_{X,Y}(x,y)}{p_X(x) p_Y(y)}\right) dx \, dy
\end{align}

for the continuous case in addition to the following properties:
\begin{align}
\label{eq:mutualcontprops}
I(X,Y) &= h(X) - h(X|Y)\\
&= h(Y) - h(Y|X)\\
%&= h(X) + h(Y) - h(X,Y) 
&= I(Y,X) \ge 0
\end{align}

Note that 
$I(X_1,X_2)=0$ if the the random variables are statistically independent. But we won't make much use of this special case equality in ICA.


\newpage

\subsection{Relationship between the KL-Divergence and Mutual Information}

Not really another approach. This is just to show how to arrive at the same expression as in the previous approach.

Recall the definition of the mutual information between input and output variables from \eqref{eq:mutualcont2}:

\begin{equation*}
I(X,Y) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} p_{X,Y}(x,y) \, \log \left(\frac{p_{X,Y}(x,y)}{p_X(x) p_Y(y)}\right) dx \, dy
\end{equation*}

and the definition for the KL divergence from \eqref{eq:klcont}:

\begin{align*}
D_{KL}\lbrack\,p\, ||\, q\,\rbrack 
&= \int_{-\infty}^{\infty} p(x) \log \left( \frac{p(x)}{q(x)} \right) dx.
\end{align*}

We can deduce that:
$$
I(X,Y) = D_{KL} \lbrack \, p_{X,Y}(x,y) \, || \, p_X(x) p_Y(y) \, \rbrack
$$

Although $I(X,Y) = I(Y,X)$ the same cannot be said about switching the arguments of the KL-divergence.

$
D_{KL} \lbrack \, p_{X,Y}(x,y) \, || \, p_X(x) p_Y(y) \, \rbrack \ne 
D_{KL} \lbrack \, p_X(x) p_Y(y) \, || \, p_{X,Y}(x,y)\, \rbrack
$

They are only equal when both of them are equal to zero, which is satisfied iff $p_{X,Y}(x,y) = p_X(x) p_Y(y)$.

This tells us that the mutual information $I(X,Y)$ is equivalent to the KL-divergence between the joint distribution $p_{X,Y}(x,y)$ and 
the product of the pdfs $p_X(x)$ and $p_Y(y)$. A special case of this is what we saw in the first approach, namely the KL-divergence 
between the pdf of a random vector $\widehat {\vec s}$ of length $N$ and the marginal probability density functions for $N$ elements 
$\widehat{s_i}$\footnote{A much smoother walkthrough for this can be found in 
Haykin Ch. 10.5}

\clearpage

\section{The Infomax Principle (Bell \& Sejnowski, 1995)}

\begin{itemize}
\item[\emph{Idea:}] Under certain conditions, maximizing the
  \emph{mutual information} between inputs (mixed signals $\vec x$) and outputs
  (recovered sources $\widehat{\vec s}$) of a system yields \emph{independent} outputs. 
  If sources are independent, then
  transformations of these sources are independent, too.
  
  Choosing the transformation such that their marginal distributions become uniform 
  simplifies the computations to find independent sources. i.e. choosing the transformation \\
$\widehat{u}_i = \widehat{f}_i(\widehat{s}_i)$ such that
$\widehat{P}_{u_i}(\widehat{u}_i) = \mathrm{const.}$

\end{itemize}

\underline{Recap on Density Transformations:}

The transformation is found using \emph{conservation of probability}
\begin{equation}
	\widehat{P}_{u_i}(\widehat{u}_i) d \widehat{u}_i 
	\; =  \; \widehat{P}_{s_i} (\widehat{s}_i) d \widehat{s}_i.
\end{equation}
Using the general rule for density transformations and applying it here yields
\begin{equation}
\label{eq:conservation1}
	\widehat{P}_{u_i}(\widehat{u}_i) \quad
	 =  \quad \bigg| 
		\frac{d \widehat{s}_i}{d \widehat{u}_i} \bigg| 
			 \widehat{P}_{s_i}(\widehat{s}_i) \quad
	 =  \quad \frac{1}{\big| \widehat{f}_i^{'} (\widehat{s}_i) \big|} 
		\widehat{P}_{s_i}(\widehat{s}_i)
\end{equation}
where $\left|\frac{d \widehat{s}_i}{d \widehat{u}_i} \right|$ is
called \emph{functional determinant} of the transformation
$\widehat{f}_i$.
Applying the principle of conservation of probability (equal size
areas) to find a transformation resulting in a uniformly distributed
variable with a constant density yields: 
\begin{equation}
  \label{eq:dtufs}
		\widehat{P}_{u_i} (\widehat{u}_i) \; = \;   
		 \frac{1}{\big| \widehat{f}_i^{'} (\widehat{s}_i) \big|} 
		\widehat{P}_{s_i}(\widehat{s}_i) \; \stackrel{!}{=} \; const \qquad \Rightarrow \qquad 
		 \big| \widehat{f}_i^{'} (\widehat{s}_i) \big| =  a \widehat{P}_{s_i}(\widehat{s}_i) 
\end{equation}
and therefore
\begin{equation}
\Rightarrow \widehat{f}_i (\widehat{s}_i)
 = \int\limits_{-\infty}^{\widehat{s}_i} dy\; a 
			\widehat{P}_{s_i}(y)
\end{equation}

\begin{figure}[h]
  \centering
  \includegraphics[width=12cm]{img/section2_fig15}  
  %\caption{density (pdf) and corresponding distribution function (cdf)}
  \label{fig:cdf}
\end{figure}

\newpage

\section{Approach 0: Maximizing mutual information directly}

Another way to reach the cost function is to maximize the mutual information directly:

$$
I(Y,X) = h(Y) - h(Y|X)
$$

By considering the following relationship between $Y$ and $X$:\\
$Y = g(X;w) + \mathcal{N}$, where $g(\cdot)$ is some invertible transformation parametrized by $w$ and $\mathcal{N}$ is additive noise.

By taking the derivative with respect to the parameter $w$:

$$
\frac{\partial}{\partial w} I(Y,X) = \frac{\partial}{\partial w}h(y) - 
\underbrace{\frac{\partial}{\partial w} h(Y|X)}_{= \frac{\partial}{\partial w} h(\mathcal{N}) = 0}
$$

Maximizing the $I(Y,X)$ boils down to maximizing the entropy of the output $Y$. 
We've seen how mutual information relates to the KL divergence, so we're going to use an approach 
that operates on the KL-divergence. 
Whether Infomax by maximizing entropy\footnote{if interested see Bell, A. J., \& Sejnowski, T. J. (1995). An information-maximization approach to blind separation and blind deconvolution. Neural computation, 7(6), 1129-1159.} 
or finding a factorization via the KL-divergence,
both are based on the Infomax principle and arrive at the same solution.

\clearpage

\section{Approach 1: Infomax via KL-divergence for the transformed densities}
\begin{eqnarray}
  \dkl & = & \int d \, \widehat{\vec{s}} P_{\vec{s}}(\widehat{\vec{s}}) \ln \frac{P_{\vec{s}}(\widehat{\vec{s}})}{\prod_i \widehat{P}_{s_i}(\widehat{s}_i)}\\
  & = &  \int d \, \widehat{\vec{s}} P_{\vec{s}}(\widehat{\vec{s}}) \ln 
  \frac
  {P_{\vec{s}}(\widehat{\vec{s}}) \quad \prod_i \frac{1}{f_i' (\widehat{s}_i)}}
  {\prod_i \widehat{P}_{s_i}(\widehat{s}_i) \quad \frac{1}{f_i' (\widehat{s}_i)}} \\
  & = & \int d \widehat{\vec{u}} P_{\vec{u}} (\widehat{\vec{u}}) 
  \ln 
  \frac
  {P_{\vec{u}} (\widehat{\vec{u}}) }
  {\prod_i  \widehat{P}_{u_i}(\widehat{u_i})} \\
  & = & 
  \underbrace{
	  \int d \widehat{\vec{u}} P_{\vec{u}} (\widehat{\vec{u}}) 
	  \ln 
	  {P_{\vec{u}} (\widehat{\vec{u}}) }
  }_{= -H \; \text{(negative entropy)}}\\
  & & \quad -
  \underbrace{
	  \int d \widehat{\vec{u}} P_{\vec{u}} (\widehat{\vec{u}}) 
	  \left( \ln
	  {\prod_{i=1}^{N}
	  \underbrace{ 
		\widehat{P}_{u_i}(\widehat{u_i}) 
	  }_{\substack{\text{const.\;} a \\\text{\;see \eqref{eq:dtufs}}}}} 
	  \right)
	  }_{\text{const.}}
\end{eqnarray}

This motivates the socalled \emph{Infomax principle}:
\begin{equation}\label{eq:infomax}
  H = -\int d \widehat{\vec{u}} P_{\vec{u}} (\widehat{\vec{u}})
  \ln P_{\vec{u}} (\widehat{\vec{u}}) \eqexcl \max 
\end{equation}
using the transformed estimated sources
\begin{equation}
\widehat{u}_i := \widehat{f}_i \big( \underbrace{ \vec{e}_i^\top
		\vec{W} \, \vec{x}  }_{= \widehat{s_i} } \big) 
\end{equation}

This tells us that we can produce statistally independent sources $\widehat {\vec s}$ 
by maixmizing the negative entropy of the of their transformation.\\
Now that we've found the cost function for our learning algorithm. 
How do we build something that can learn to solve it.

% -----------------------------------------------------------------------------

\newpage

\section{Empirical Risk Minimization}
Consider the following perceptron network with $N$ inputs and $N$ outputs:
\begin{figure}[ht]
\centering
\includegraphics[width=6cm]{img/section2_fig16}
%\caption{$N-N$ perceptron network}
\end{figure}
where
$$
\widehat{u}_i := \underbrace{
	\widehat{f}_i 
	\Big( \sum_{j=1}^{N} \mathrm{w}_{ij} 
		\mathrm{x}_j 
	\Big) }_{
	\substack{
		\text{often a} \\
		\text{sigmoid function}\\
		\text{or\;} \tanh}
	}
$$
and observations:
$$
\vec{x}^{(\alpha)} \in \mathbb{R}^N, 
		\quad \alpha = 1, \ldots, p
$$
The weights $w_{ij}$ will be determined through inductive learning.\\
Deriving the cost function for this network to find the Infomax solution:
\begin{equation}
\label{eq:conservationvec}
	P_{\vec{u}} (\widehat{\vec{u}}) d \widehat{\vec{u}}
		= P_{\vec{x}}(\vec{x}) d \vec{x}
\end{equation}
\begin{align}
\label{eq:uxj}
	P_{\vec{u}} (\widehat{\vec{u}}) 
	& = \left| \det \frac{d \vec{x}}{d \widehat{\vec{u}}} \right|
		P_{\vec{x}}(\vec{x}) \\
	& = \frac{P_{\vec{x}}(\vec{x})}{ \left| \det
		\frac{d \widehat{\vec{u}}}{d \vec{x}} \right|}\\
    & = \frac{P_{\vec{x}}(\vec{x})}{|\det \vec{J}\,|}
\end{align}
with elements of the Jacobian $\vec J$ given as
\begin{align}
\label{eq:jacobelement}
 J_{ij}=
 \frac{\partial \widehat{u}_i}{\partial \mathrm{x}_j}
	& = \frac{\partial}{\partial \mathrm{x}_j} 
		\widehat{f}_i \bigg( \sum\limits_{k = 1}^N \mathrm{w}_{ik} 
		\mathrm{x}_k \bigg) \\
	& = \mathrm{w}_{ij} \widehat{f}_i^{'} \bigg( \sum\limits_{k = 1}^N 
		\mathrm{w}_{ik} \mathrm{x}_k \bigg).
\end{align}
We therefore obtain for the value of the Jacobian determinant
\begin{equation} \label{eq:functionalDeterminant}
|\det \vec {J}\,| = 
	\Big| \det \frac{\partial \widehat{\vec{u}}}{\partial \vec{x}} \Big|
	= |\det \vec{W}\, | \prod\limits_{l = 1}^N  \widehat{f}_l^{'} \Bigg( 
		\sum\limits_{k = 1}^N \mathrm{w}_{lk} \mathrm{x}_k \Bigg).
\end{equation}

\clearpage

Inserting \eqref{eq:conservationvec} and \eqref{eq:uxj} into the Infomax cost function from \eqref{eq:infomax} gives 
\begin{eqnarray}
H & = & -\int d \widehat{\vec{u}} P_{\vec{u}} (\widehat{\vec{u}})
  \ln P_{\vec{u}} (\widehat{\vec{u}}) \\
& = &  
-\int d \vec{x} P_{\vec{x}} (\vec{x}) \ln \frac{P_{\vec{x}}(\vec{x})}{|\det \vec{J}\,|} \\
& = & 
\underbrace{
    -\int d \vec{x} P_{\vec{x}} (\vec{x}) \ln P_{\vec{x}}(\vec{x})
}_{ \text{constant w.r.t. } \vec W } 
    + \int d \vec{x} P_{\vec{x}} (\vec{x}) \ln |\det \vec{J}\,|
\end{eqnarray}
and with \eqref{eq:functionalDeterminant} we can formulate the cost 
in terms that explicitly depend on $\vec W$ and its components:
\begin{equation}
	H = const. \, + \; \ln |\det \vec{W}\,| \underbrace{\int d \vec{x} P_{\vec{x}} (\vec{x})}_{=\,1}
		+ \int d \vec{x} P_{\vec{x}} (\vec{x}) \sum\limits_{l = 1}^N
			\ln \widehat{f}_l^{'} \Bigg( \sum\limits_{k = 1}^N 
			\mathrm{w}_{lk} \mathrm{x}_k \Bigg).
\end{equation}
This enables us to define the generlization cost $E^G$ for model selection:
\begin{equation} \tag{generalization cost}
	E^G = \ln |\det \vec W\,| + \int d \vec{x} P_{\vec{x}} (\vec{x})
		\Bigg\{ \sum\limits_{l = 1}^N \ln
			\widehat{f}_l^{'} \Bigg( \sum\limits_{k = 1}^N 
			\mathrm{w}_{lk} \mathrm{x}_k \Bigg)
		\Bigg\}
\end{equation}
The \emph{principle of empirical risk minimization} (in our particular case maximization) allows
\begin{center}
mathematical expectation $E^G \longrightarrow$ empirical average $E^T$
\end{center}
the \emph{training cost}
\begin{equation} \label{eq:trainingCost}
	E^T = \ln |\det \vec{W}\,| + \frac{1}{p} \sum\limits_{\alpha = 1}^p
		\sum\limits_{l = 1}^N \ln \widehat{f}_l^{'} \Bigg( 
		\sum\limits_{k = 1}^N \mathrm{w}_{lk} 
		\mathrm{x}_k^{(\alpha)} \Bigg)
\end{equation}
can be used for model selection using empirical data 
\begin{equation}
E^T \eqexcl \max
\end{equation}

\newpage

\subsection{Learning by Gradient Ascent (i.e. hill climbing)}
Model parameters can be optimized by stepwise adjustment along the direction of the gradient of the cost function. 

\begin{figure}[h]
  \centering
  \begin{tabular}[c c]{c c}
   \includegraphics[width=5cm]{img/section2_fig17}
  &\raisebox{2cm}{$\Delta \mathrm{w}_{ij} = \underbrace{ \eta }_{
    \substack{ \text{learning} \\ \text{rate}} }
  \frac{\partial E^T}{\partial \mathrm{w}_{ij}}$}
  \end{tabular}  
  %\caption{Gradient ascent using the training cost}
  \label{fig:gradientDescent}
\end{figure}
\noindent Taking partial derivatives of the training cost in \eqref{eq:trainingCost} w.r.t. the model parameters $w_{ij}$ yields
\begin{equation}
	\frac{\partial E^T}{\partial \mathrm{w}_{ij}}
	= \underbrace{
    \frac{1}{p} \sum\limits_{\alpha = 1}^p 
		\sum\limits_{l = 1}^N \frac{\partial}{\partial \mathrm{w}_{ij}}
		\Bigg\{ \ln \widehat{f}_l^{'} \Bigg( \sum\limits_{k = 1}^N 
		\mathrm{w}_{lk} \mathrm{x}_k^{(\alpha)} \Bigg) \Bigg\}
        }_{ =
			\frac{1}{p} \sum\limits_{\alpha = 1}^p 
			\frac{ \widehat{f}_i^{''} \Big( \sum\limits_{k = 1}^N 
				\mathrm{w}_{ik} \mathrm{x}_k^{(\alpha)} \Big)
			}{\widehat{f}_i^{'} \Big( \sum\limits_{k = 1}^N 
			\mathrm{w}_{ik} \mathrm{x}_k^{(\alpha)} \Big)}
			\cdot \mathrm{x}_j^{(\alpha)} }
		+ \underbrace{ \frac{\partial}{\partial w_{ij}}
			\big( \ln |\det \vec{W}\,| \big) }_{
				\big( \vec{W}^{-1} \big)_{ji} }
\end{equation}
with an individual cost $e^{(\alpha)}$ for each observation $\mathrm{x}^{(\alpha)}$:
\begin{equation}
	e^{(\alpha)} = \ln |\det \vec{W}\,| + \sum\limits_{l = 1}^N \ln
		\widehat{f}_l^{'} \Bigg( \sum\limits_{k = 1}^N 
		\mathrm{w}_{lk} \mathrm{x}_k^{(\alpha)} \Bigg)
\end{equation}
\begin{equation}
	\frac{\partial e^{(\alpha)}}{\partial \mathrm{w}_{ij}}
	= \underbrace{ \big( \vec{W}^{-1} \big)_{ji} }_{
		\substack{ \text{costly} \\ \text{computation}} }
		+ \underbrace{  
			\frac{ \widehat{f}_i^{''} \bigg( \sum\limits_{k = 1}^N 
				\mathrm{w}_{ik} \mathrm{x}_k^{(\alpha)} \bigg)
			}{\widehat{f}_i^{'} \bigg( \sum\limits_{k = 1}^N 
			\mathrm{w}_{ik} \mathrm{x}_k^{(\alpha)} \bigg)}
			 }_{ \coloneqq \varphi_i^{(\alpha)} }
		\cdot \mathrm{x}_j^{(\alpha)}
\end{equation}
this can be used for \emph{batch-learning}:
\begin{equation}
	\Delta \mathrm{w}_{ij}
	= \frac{\eta}{p} \sum\limits_{\alpha = 1}^p 
	\frac{\partial e^{(\alpha)}}{\partial \mathrm{w}_{ij}}
\end{equation}
or using \emph{on-line-learning} by updating $w_{ij}$ with each individual cost $e^{(\alpha)}$ as follows:
\begin{algorithm}[ht]
  \DontPrintSemicolon
  $t \leftarrow 1$\;
  random initialization of weights $w_{ij}$\;
  \Begin{
    $\eta_t = \frac{\eta_0}{t}$\;
    select next data point $\vec{x}^{(\alpha)}$\;
    change all  $\mathrm{w}_{ij}$ according to:
    $\Delta \mathrm{w}_{ij}^{(t)} = \eta_t \frac{\partial e_t^{(\alpha)}}{\partial
	\mathrm{w}_{ij}} $\;
    $t \leftarrow t + 1$}
%\caption{On-line learning for ICA}
\label{alg:onlineGD}
\end{algorithm}

\clearpage

\subsection{Natural Gradient Learning}
The natural gradient allows for an efficient \& fast learning rule (no matrix inversions
  necessary!) to do steepest ascent under normalized step size (cf. lecture slides 2.2.1 for details)

Linear transformations: $d \vec{W}, \vec{W}$
\begin{figure}[h]
  \centering
\includegraphics[width=10cm]{img/section2_fig18}  
  %\caption{Illustration of gradient descent in transformed coordinate system}
  \label{fig:NatGrad}
\end{figure}


% -----------------------------------------------------------------------------
\newpage

%\subsection{Choice of $\widehat{f}_i$:}
The true distribution is typically unknown, 
but likely to have a probability density with one maximum (i.e. peaky function)
$\leadsto$ cdf of this unknown source distribution will be roughly sigmoidal
%\\\\
Typical choice to resemble the cdf of such a peaky function:
\begin{equation} \tag{logistic function}
	\widehat{f}_{(y)} = \frac{1}{1 + \exp(-y)}
\end{equation}
\begin{equation}
	\frac{\widehat{f}_{(y)}^{''}}{\widehat{f}_{(y)}^{'}}
	= 1 - 2 \widehat{f}_{(y)}
\end{equation}
Observation: ICA is fairly robust against false choice of $\widehat{f}$.

\begin{itemize}
	\itR however: if $\widehat{f}_i$ deviates too strongly from its true
		shape, the fixed point may become unstable
	\itR if in doubt (and enough training data is available)
	\begin{itemize}
		\itl make a parametrized ansatz for $\widehat{f}_i$
		\itl estimate parameters in addition to $\vec{W}$
	\end{itemize}
\end{itemize}

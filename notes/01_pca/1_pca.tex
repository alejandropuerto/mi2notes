\section{PCA}

\begin{enumerate}
\item Center the data, $\E[\vec x] = \vec m  = \frac{1}{p} \sum_{\alpha=1}^{p} \vec x^{(\alpha)}\eqexcl \vec 0$.
\item Let $\vec X'$ be the $N \times p$ matrix of the centered data.
\item Measure the variance of each component in $\vec x_{centered}$. Not enough, the variables in $\vec x$ could be correlated.
\item Measure covariances $C_{ij}$.
\item Construct covariance matrix $\vec C$.\\
$$ 
\vec C = \text{Cov}(\vec X') = \mathbf{\Sigma} = \E[\vec X' \vec X'^\top] \in \R^{N \times N}
$$
\item \textbf{eigenvalue decomposition}:\\
$$
\vec C \, \vec e_a \; = \; \lambda_a \vec e_a  \quad\text{(the eigenvalue problem of PCA)}
$$
$$
\lambda_a: \text{the eigenvalue, the variance along principle component } a.
$$
$$
\vec e_a: \text{the normalized eigenvector, the direction of PC } a \text{ in } R^N
$$

\question{How do we perform an eigenvalue decomposition?}
((1) Get eigenvalues: $\det(\vec C-\lambda \vec I) = 0$, 
(2) Find eigenvector $\vec e_a$ associated with each $\lambda_a$ by solving the linear system\\
$ (\vec C - \lambda_a \vec I )\, \vec e_a = \vec 0$
(see {\emph{math\_primer.pdf} on ISIS}  for details)
\item Order eigenvalues in descending order. (Highest variance first). The ordered eigenvectors are the \emph{principle components} of the dataset $\vec X$.

\question{What is a scree plot?}

\item Rotate $\vec x_{centered}$ using the first $M$ PCs.
\end{enumerate}

%\newpage

\underline{How much better is this than simple trunctation?}\\[0.3cm]
The transformation onto the PCs is linear:
\begin{equation*}
	\vec{x} = \underbrace{ a_1 }_{ \vec{e}_1^\top \vec{x} } \vec{e}_1
		+ \underbrace{ a_2 }_{ \vec{e}_2^\top \vec{x} } \vec{e}_2
		+ \ldots
		+ \underbrace{ a_N }_{ \vec{e}_N^\top \vec{x} } \vec{e}_N
\end{equation*}

Therefore, we perfectly reconstruct observations by projecting them from PC space (i.e. feature space) back to the input space. 
However, if we only use the first $M$ PCs for reconstructing the observations, we will accumulate error.

$$
\widetilde{\vec{x}} = a_1 \vec{e}_1 + a_2 \vec{e}_2 + \ldots
		+ a_M \vec{e}_M
$$
		
We measure MSE between original centered observations and their corresponding reconstructions.
$$
E = \frac{1}{p} \sum_{\alpha = 1}^{p} (\vec x_{centered} - \widetilde{\vec{x}})^2 = \frac{1}{p} \sum_{\alpha = 1}^{p} \sum_{j = M+1}^{N} (a_j^{(\alpha)})^2
$$
The MSE is equal to the sum of variances of the final M components of the \emph{transformed} observations. Since we've ordered the PCs w.r.t to variance in descending order the variance of the last $M-N$ components of the transformed data is smallest.
The transformation is therefore optimal in the sense of minimal MSE.



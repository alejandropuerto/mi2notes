\subsection{Applying PCA}

\mode<presentation>{
\begin{frame} 
    %\begin{center} \huge
        %\subsecname
    %\end{center}
    \begin{center}
		\includegraphics[width=5cm]{img/meme_knowpca}
    \end{center}
    \begin{center}What can we do with it?
    \end{center}
\end{frame}
}


\begin{frame}\frametitle{\subsecname}

\begin{enumerate}
\item Dimensionality reduction
\item Visualization (e.g. plot the projections onto the first 2 or 3 PCs.)
\item Outlier detection (project onto PCs with \emph{smallest} eigenvalues)
\item Whitening (i.e. decorrelate the data) - exploit that eigenvectors form an orthonormal basis.
$$
\vec v^{(\alpha)} = \vec x_{whitened}^{(\alpha)} = \vec{\Lambda}^{-\frac{1}{2}}\vec{M}^\top\vec{x}^{(\alpha)}
\quad
\forall \alpha
$$

\question{What is the covariance of $\vec x_{whitened}$?}

\pause

-the Identity matrix.

\end{enumerate}

\end{frame}

\newpage


\subsubsection{A note on implementation:}

\begin{frame}\frametitle{\subsubsecname}

\question{Is an off-the-shelf implementation such as\\ sklearn.decomposition.PCA for python really necessary?}

\notesonly{
PCA boils down to solving the eigenvalue problem.
We bascially need a routine with which to solve the eigenvalue problem. 
}
\pause

    \begin{center}
		\includegraphics[width=5cm]{img/meme_eig}
    \end{center}
    
\pause

For python, the obvious choice is to use a routine like \textit{numpy.linalg.eig(A)}. \emph{Watch out for how to intepret the output.}

\end{frame}

\begin{frame}{Only}\frametitle{\subsubsecname}

\svspace{-5mm}

\question{Are there alternatives to eigenvalue decomposition?}

\notesonly{Yes, namely,}

\underline{Singular Value Decomposition (SVD)}:\\
Let $\vec X' \in \R^{N \times p}$ be the centered dataset (centering is a must):
\begin{equation}
\vec X'_{N \times p} = \vec U_{N \times N} \, \vec S_{N \times p} \, \vec V^\top_{{p \times p}}
\end{equation}
\svspace{-5mm}
where,
\begin{equation}
\vec U^\top \vec U = \vec I_{N \times N}
\notesonly{
\end{equation} and 
\begin{equation}
}
\slidesonly{\quad \text{and} \quad}
\vec V^\top \vec V = \vec I_{p \times p}
\end{equation} (i.e. $\vec U$, $\vec V$ are orthogonal).
\begin{itemize}

\item<only@2> The columns of $\vec V$ (or rows of $\vec V^\top$) make up the eigenvectors of $\vec X'^\top\vec X'$.
\item<only@3,4,5> The columns of $\vec U$ make up the eigenvectors of $\vec X'\vec X'^\top$.
\item<only@4,5> $\vec S$ is a diagonal matrix. The singular values in $\vec S$ (the diagonal entries) are the \textbf{square roots} of the  eigenvalues of the eigenvectors of $\vec X'\vec X'^\top$ or $\vec X'^\top\vec X'$.
\end{itemize}

\only<5>{
Recall:
$$
\vec C = \E[\vec X' \vec X'^\top] = \frac{1}{p} \vec X' \vec X'^\top
$$
}

\end{frame}

\begin{frame}\frametitle{\subsubsecname}

\question{When to use SVD vs. Eig? \textbf{Not dogma}}

\begin{itemize}

\item Numerical instabilities in eigendecomposition of very large covariance matrix $\rightarrow$ SVD
\item SVD not applicable to Kernel-PCA $\rightarrow$ Eig
\item Computational efficiency $\rightarrow$ SVD (depends...?)
\end{itemize}

\slidesonly{
$$
\vec X' \in \R^{N \times p} \quad \leadsto \quad \vec C = \frac{1}{p} \vec X' \vec X'^\top
$$
}

\end{frame}

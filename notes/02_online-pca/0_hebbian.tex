
\section{PCA: Recap}

\begin{frame}{\secname}

\begin{itemize}
\item requires centering the data
\item eigendecomposition of $\text{Cov}(\vec X)$, alternatively: SVD($\vec X$)
\item sensitive to the scales of the individual variables
\item deterministic, insensitive to the order of the observations
\item Applications: dimensionality reduction, outlier detection, whitening
\item limited to linear correlations
\item will be referred to as ``standard'' or ``batch'' PCA.
\end{itemize}

\pause

\question{What are the implications of computing PCA on \emph{non-stationary} data?}

\pause

- We need an online method for computing the directions of highest variance that adapts to possible changes in this direction over time.

\end{frame}

\section{Hebbian Learning}

\mode<presentation>{
\begin{frame} 
    \begin{center} \huge
        \secname
    \end{center}
	\begin{center}
	\textit{``Neurons that fire together wire together.''} - Donald Hebb
	\end{center}
\end{frame}
}

\mode<article>{
\begin{center}
\textit{``Neurons that fire together wire together.''} - Donald Hebb
\end{center}
}

\begin{frame}{\secname}

\begin{itemize}
\item[] observations: $\big\{ \vec{x}^{(\alpha)} \big\}, \alpha = 1, \ldots, p; \quad \vec{x} \in \R^N$\\

\item[] assumption: centered data\\

\item[] response: $y = \sum_{i=1}^{N} w_i x_i = \vec w^{\top} \vec x$\\

\item[] We don't really have a desired output for the network.\\
\item[] However, we can interpret its response $y$.
The inner product yields a higher value whenever $\vec x$ is ``close'' to $\vec w$.
Therefore, $y$ measures similarity between $\vec x$ and $\vec w$.
A large value of $y$ means that the network ``recognizes'' the stimulus $\vec x$.
\end{itemize}

\end{frame}

\newpage

\subsection{Motivation for Hebbian learnig}

\begin{frame}{\subsecname}

\begin{itemize}
\item We can use this to build associative memory (i.e. correlation learning).
\item Applicable to non-stationary data.
\item More importantly, a single linear neuron 
can organize itself such that its synaptic weights converge to a filter 
for the \slidesonly{first PC}\notesonly{\emph{first principle component}} (Oja, 1982).

\pause

\mode<presentation>{
    \begin{center}
		\includegraphics[width=5cm]{img/meme_hebbian}
    \end{center}

}

\end{itemize}

\end{frame}

\subsection{The Hebbian learning rule}

\begin{frame}{\subsecname}
\begin{equation}
\vec w(t+1) = \vec w(t) + \underbrace{\varepsilon~y(\vec x^ {(\alpha)}; \vec w(t)) \vec x^{(\alpha)}}_{= \Delta \vec w}
\end{equation}
where $\varepsilon$ is the \emph{learning rate}.\\

\underline{The consequences of using this update rule in simulation:}\\

\begin{enumerate}
\item Divergence.\\
As $t \rightarrow \infty$ \qquad $\leadsto |\vec{w}| \rightarrow \infty$\\

\pause 

\notesonly{
\question{How does nature deal with the divergence problem?}
}
\mode<presentation>{
\only<2>{
    \begin{center}
		\includegraphics[width=5cm]{img/meme_diverges}
    \end{center}
}
}

\pause

\item $\vec{e}_{\mathrm{w}} = \frac{\vec{w}}{|\vec{w}|}$ 
\end{enumerate}

\end{frame}

